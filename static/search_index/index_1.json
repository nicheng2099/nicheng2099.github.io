{"/doc2/六、Hadoop安装.html":{"title":"Hadoop安装","content":"# Hadoop安装 ### 安装 ##### 访问hadoop官网中的文档 [Apache Hadoop 3.3.5 – Hadoop: Setting up a Single Node Cluster.](https://hadoop.apache.org/docs/stable/hadoop project dist/hadoop common/SingleCluster.html) ![image 20230608154707652](image/image 20230608154707652.png) ##### 到opt下的soft下，并放入hadoop ![image 20230608155250848](image/image 20230608155250848.png) ![image 20230608155315396](image/image 20230608155315396.png) ##### 解压到/opt/apps ``` tar zxvf ./hadoop 2.7.2.tar.gz C /opt/apps/ ``` ![image 20230608155443485](image/image 20230608155443485.png) ![image 20230608155514802](image/image 20230608155514802.png) ### 配置文件 ##### 到/opt/apps/hadoop 2.7.2/etc/hadoop下编辑hadoop env.sh > 添加jdk地址 /opt/apps/jdk1.8.0_181/ ``` vim /opt/apps/hadoop 2.7.2/etc/hadoop/hadoop env.sh ``` ![image 20230608160146853](image/image 20230608160146853.png) ##### 编辑core site.xml > HDSF服务在LG04的9000端口上，hadoop.tmp.dir，临时文件夹的路径 ``` vim core site.xml <property> <name>fs.defaultFS</name> <value>hdfs://LG04:9000</value> </property> <property> <name>hadoop.tmp.dir</name> <value>/opt/apps/hadoop 2.7.2/data/tmp</value> </property> ``` ![image 20230608160822899](image/image 20230608160822899.png) ##### 编辑hdfs site.xml > dfs.replication的值设为3 ，代表3个副本，网页部署到LG05的50090端口上， ``` vim hdfs site.xml <property> <name>dfs.replication</name> <value>3</value> </property> <property> <name>dfs.namenode.secondary.http address</name> <value>LG05:50090</value> </property> <property> <name>dfs.namenode.http address</name> <value>LG04:50070</value> </property> <property> <name>dfs.permissions.enabled</name> <value>false</value> </property> ``` ![image 20230608165036013](image/image 20230608165036013.png) ##### 改名并编辑mapred site.xml ``` mv ./mapred site.xml.template ./mapred site.xml vim ./mapred site.xml <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> ``` ![image 20230608165601204](image/image 20230608165601204.png) ##### 修改yarn site.xml ``` vim yarn site.xml <configuration> <! Site specific YARN configuration properties > <property> <name>yarn.nodemanager.aux services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.resourcemanager.hostname</name> <value>LG06</value> </property> <property> <name>yarn.nodemanager.resource.memory mb</name> <value>2048</value> </property> <property> <name>yarn.log aggregation enable</name> <value>true</value> </property> <property> <name>yarn.log aggregation.retain seconds</name> <value>604800</value> </property> </configuration> ``` ![image 20230608170833215](image/image 20230608170833215.png) ##### 修改slaves ``` vim slaves ``` ![image 20230609140414107](image/image 20230609140414107.png) ##### 发送hadoop给LG05、LG06（在apps目录下） ``` scp r ./hadoop 2.7.2/ LG05:/opt/apps/ scp r ./hadoop 2.7.2/ LG06:/opt/apps/ ``` ![image 20230609140757765](image/image 20230609140757765.png) ### 格式化 ##### 关闭防火墙（三台都要） ``` service network status service iptables status ``` ![image 20230609142620459](image/image 20230609142620459.png) ``` service iptables stop ``` ![image 20230609142704973](image/image 20230609142704973.png) ``` chkconfig iptables off //关机后也保持防火墙关闭 ``` ![image 20230609142742547](image/image 20230609142742547.png) ##### 到/opt/apps/hadoop 2.7.2/bin目录下格式化 ``` ./hdfs namenode format ``` ![image 20230609143151333](image/image 20230609143151333.png) ##### 启动/opt/apps/hadoop 2.7.2/sbin/start dfs.sh ``` /opt/apps/hadoop 2.7.2/sbin/start dfs.sh jps ``` ![image 20230609144010576](image/image 20230609144010576.png) ##### 在LG05 ``` jps ``` ![image 20230609144350264](image/image 20230609144350264.png) ##### 在LG06 /opt/apps/hadoop 2.7.2/sbin/目录下执行./start yarn.sh ``` /opt/apps/hadoop 2.7.2/sbin/start yarn.sh ``` ![image 20230609145105997](image/image 20230609145105997.png) ##### 返回LG05 ``` jps ``` ![image 20230609145331040](image/image 20230609145331040.png)"},"/doc2/十、sqoop的操作.html":{"title":"sqoop的操作","content":"# sqoop的操作 ## sqoop的导入 ### 创建数据库（无主键，一个map） ##### 进入MySQL的test数据库 ``` mysql uroot proot show databases; use test; show tables; ``` ![image 20230613151813636](image/image 20230613151813636.png) ![image 20230613151834877](image/image 20230613151834877.png) ##### 创建表 ``` create table nba(id int(4),name varchar(20),num int(4),team varchar(20)); ``` ![image 20230613152153649](image/image 20230613152153649.png) ##### 插入数据 ``` insert into nba values(1,\"科比\",42,\"湖人\"),(2,\"詹姆斯\",37,\"湖人\"),(3,\"库里\",32,\"勇士\"),(4,\"哈登\",32,\"火箭\"); ``` ![image 20230613152500735](image/image 20230613152500735.png) ### 使用sqoop导入hdf（无主键，一个map） ##### 到/opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin目录下 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin ``` ##### 执行导入 > 因为nba表中无主键 m不能>1 ``` ./sqoop import \\ > connect jdbc:mysql://LG04:3306/test \\ > username root \\ > password root \\ > table nba \\ > target dir /data/to_hdfs \\ > m 1 ``` ``` ./sqoop import connect jdbc:mysql://LG04:3306/test username root password root table nba target dir /data/to_hdfs m 1 ``` ![image 20230613160539871](image/image 20230613160539871.png) ![image 20230613160553186](image/image 20230613160553186.png) ### 创建数据库（有主键，多个map） ##### 创建表 ``` create table nba_key(id int(4) primary key,name varchar(20),num int(4),team varchar(20)); ``` ![image 20230613161144972](image/image 20230613161144972.png) ##### 插入数据 ``` insert into nba_key values(1,\"科比\",42,\"湖人\"),(2,\"詹姆斯\",37,\"湖人\"),(3,\"库里\",32,\"勇士\"),(4,\"哈登\",32,\"火箭\"); ``` ![image 20230613161329875](image/image 20230613161329875.png) ### 使用sqoop导入hdf（有主键，多个map） ##### 到/opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin目录下 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin ``` ##### 执行导入 > map可以为多个,结果会有多个part文件拆分数据 ``` ./sqoop import \\ > connect jdbc:mysql://LG04:3306/test \\ > username root \\ > password root \\ > table nba_key \\ > target dir /data/to_hdfs_nba_key \\ > m 4 ``` ``` ./sqoop import connect jdbc:mysql://LG04:3306/test username root password root table nba_key target dir /data/to_hdfs_nba_key m 4 ``` ![image 20230613161916985](image/image 20230613161916985.png) ![image 20230613161938763](image/image 20230613161938763.png) ## sqoop的导出 ##### 建一张空表 > 用于接收导出的数据 ``` create table nba_result(id int(4) primary key,name varchar(20),num int(4),team varchar(20)); ``` ![image 20230613162914343](image/image 20230613162914343.png) ##### 进入/opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin目录下 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin ``` ##### 执行导出 > 没有必要指定m，有几个文件，就指定几个map ``` ./sqoop export \\ > connect jdbc:mysql://LG04:3306/test \\ > username root \\ > password root \\ > table nba_result \\ > export dir /data/to_hdfs_nba_key \\ > fields terminated by ',' \\ > lines terminated by '\\n' ``` ![image 20230613163931923](image/image 20230613163931923.png) ## 增量导入 > 必须要在有主键中的表中实现 ##### 在nba_key表中再添加4条数据 ``` insert into nba_key values(5,\"杜兰特\",31,\"篮网\"),(6,\"欧文\",30,\"独行侠\"),(7,\"东七七\",26,\"独行侠\"),(8,\"易建联\",36,\"宏远\"); ``` ![image 20230613164507662](image/image 20230613164507662.png) ##### 进入/opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin目录下 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin ``` ##### 执行导入 ``` ./sqoop import \\ > connect jdbc:mysql://LG04:3306/test \\ > username root \\ > password root \\ > table nba_key \\ > target dir /data/to_hdfs_nba_key \\ > check column id \\ > incremental append \\ > last value 4 ``` ``` ./sqoop import connect jdbc:mysql://LG04:3306/test username root password root table nba_key target dir /data/to_hdfs_nba_key check column id incremental append last value 4 ``` ![image 20230613165438247](image/image 20230613165438247.png) ## 增量导出 > 将增量增加的文件导出给mysql ##### 进入/opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin目录下 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin ``` ##### 执行导出 > 将part m 00004导入nba_result表 ``` ./sqoop export connect jdbc:mysql://LG04:3306/test username root password root table nba_result export dir /data/to_hdfs_nba_key/part m 00004 fields terminated by ',' lines terminated by '\\n' ``` ![image 20230613170252036](image/image 20230613170252036.png) ![image 20230613170206574](image/image 20230613170206574.png)"},"/doc2/八、hive的相关操作.html":{"title":"hive的相关操作","content":"# hive的相关操作 > 在启动hive前要启动hadoop ``` /opt/apps/hadoop 2.7.2/sbin/start dfs.sh /opt/apps/hadoop 2.7.2/sbin/start yarn.sh ``` ##### 启动hive > 进入cd /opt/apps/apache hive 1.2.2 bin/bin/\t执行./hive service metastore& ``` cd /opt/apps/apache hive 1.2.2 bin/bin/ ./hive service metastore& ``` ![image 20230612141652579](image\\image 20230612141652579.png) ``` ./hive ``` ![image 20230612141749439](image\\image 20230612141749439.png) ##### 创建并使用数据库 ``` create database lc; use lc; ``` ![image 20230612142117380](image\\image 20230612142117380.png) ![image 20230612142146930](image\\image 20230612142146930.png) ##### 创建student表 ``` create table student(id int,name string); show tables; ``` ![image 20230612142615436](image\\image 20230612142615436.png) ![image 20230612142708416](image\\image 20230612142708416.png) ##### 添加数据 ``` insert into student values(1,\"mike\"),(2,\"kobe\"),(3,\"james\"); ``` ![image 20230612143112049](image\\image 20230612143112049.png) ![image 20230612150006626](image/image 20230612150006626.png) ![image 20230612143121579](image\\image 20230612143121579.png) ##### 查询表 ``` select * from student; ``` ![image 20230612143342297](image\\image 20230612143342297.png) ##### 处理中文乱码 > ctrl C\t//退出hive > > jps\t//查询Runjar进程号 > > kill 9 Runjar进程\t//杀死Runjar进程 ![image 20230612164658898](image/image 20230612164658898.png) ##### 进入mysql中的hive数据库 ``` mysql uroot proot show databases; use hive; ``` ![image 20230612165206019](image/image 20230612165206019.png) ##### 依次输入执行以下sql语句 ``` alter table COLUMNS_V2 modify column COMMENT varchar(256) character set 'utf8'; alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set 'utf8'; ``` ``` alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set 'utf8'; alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set 'utf8'; ``` ``` alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set 'utf8'; ``` ![image 20230612165416864](image/image 20230612165416864.png) ##### 退出mysql并重启mysql和hive ``` service mysql restart ./hive service metastore& ``` ![image 20230612165653824](image/image 20230612165653824.png) ##### 再次进入hive ``` ./hive ``` ##### 建表 > 以','进行分割 ``` create table man(id int,name string,age int,address string)row format delimited fields terminated by ','; ``` ##### 插入数据 ``` insert into man values(1,\"LZX\",48,\"TW\"),(2,\"LYF\",35,\"SC\"),(3,\"WYF\",32,\"Canada\"),(4,\"LYD\",40,\"CQ\"); ``` ![image 20230613122643022](image/image 20230613122643022.png) ![image 20230613122659710](image/image 20230613122659710.png) ![image 20230613124002920](image/image 20230613124002920.png) ##### 传入car.txt文件，假设先有文件，再有数据表 > 该表的分隔符是“\\t” ![image 20230613123936150](image/image 20230613123936150.png) ##### ![image 20230613124155399](image/image 20230613124155399.png) ##### 对car.txt中的数据建表 > 品牌 Brand\t\t\t\t\t\t\t\t\t\t(string) > 下属品牌\tSubordinate_brands\t(string) > 车系 Car_series\t\t\t\t\t\t\t\t (string) > 在售情况\tOn_sale_situation\t\t (string) > 参数配置\tConfiguration\t\t\t\t(string) > 经销商报价 Dealer_quotation \t\t(int) > 型号 Model\t\t\t\t\t\t\t\t\t\t(string) > 指导价 Guide_price\t\t\t\t\t\t (int) > 用户评分\tUser_rating\t\t\t\t\t(float) > 车型 Vehicle_model\t\t\t\t\t\t (string) > 车身结构\tBody_structure\t\t\t (string) > 发动机 Engine\t\t\t\t\t\t\t\t\t(string) > 变速箱 Transmission\t\t\t\t\t\t(string) ``` create table car(Brand string,Subordinate_brands string,Car_series string,On_sale_situation string,Configuration string,Dealer_quotation int,Model string,Guide_price int,User_rating float,Vehicle_model string,Body_structure string,Engine string,Transmission string)row format delimited fields terminated by '\\t'; ``` ##### （了解）本地操作：将文件car.txt中的数据上传到car表中 ``` load data local inpath \"/opt/apps/car.txt\" into table car; ``` ![image 20230613134956583](image/image 20230613134956583.png) ##### 将文件cai.txt上传到hadoop上再将数据上传到car表中 > 进入/opt/apps/hadoop 2.7.2/bin目录执行上传操作，上传到根目录 ``` cd /opt/apps/hadoop 2.7.2/bin ./hdfs dfs put /opt/apps/car.txt / ``` ![image 20230613135449965](image/image 20230613135449965.png) ![image 20230613135545541](image/image 20230613135545541.png) ##### 上传数据到car表 > 与上传本地相比，删除local ，代表默认不是本地地址，而是hadoop地址 ``` load data inpath \"/car.txt\" into table car; select * from car; ``` ![image 20230613135831619](image/image 20230613135831619.png) ![image 20230613135851044](image/image 20230613135851044.png) ![image 20230613135905451](image/image 20230613135905451.png) ![image 20230613135924303](image/image 20230613135924303.png) ### hive的相关作业 ##### 2，计算每个车辆品牌的数量分别是多少 （落地成新的表格 表名car_p_count 字段名(car_p > 车辆品牌，car_count > 车辆数量)） ``` create table car_p_count as select Brand as car_p,COUNT(*) as car_count from car group by Brand; ``` ##### 3，计算车辆在售情况统计 （落地成新的表格 表名car_sell 字段名(car_sell_status > 车辆在售状态，car_count > 车辆数量)）（15分） ``` create table car_sell as select On_sale_situation as car_sell_status,COUNT(*) as car_count from car group by On_sale_situation; ``` ##### 4，计算车辆各个品牌售价总和 （落地成新的表格 表名car_price 字段名(car_p > 车辆品牌，car_total_price > 车辆总价格)） （20分） ``` create table car_price as select Brand as car_p,sum(Dealer_quotation) as car_total_price from car group by Brand; ``` ##### 5，计算车辆除去无评分状态下各个品牌评价总和排名[降序] （落地成新的表格 表名car_f 字段名(car_p > 车辆品牌，car_total_f > 车辆总评分)）（20分） ``` create table car_f as select Brand as car_p,sum(User_rating) as car_total_f from car where User_rating is NOT NULL group by Brand order by car_total_f DESC; ``` ##### 6，（附加）计算各种车型平均报价并排序[升序] （落地成新的表格 表名car_xin 字段名(car_x > 车辆车型，car_a_p > 车辆平均报价)）（20分） ``` create table car_xin as select Vehicle_model as car_x,avg(Dealer_quotation) as car_a_p from car group by Vehicle_model order by car_a_p ASC; ```"},"/doc2/二、配置主机名并添加ip映射.html":{"title":"配置主机名并添加ip映射","content":"# 配置主机名并添加ip映射 ## 修改主机名 ``` vim /etc/sysconfig/network ``` ​\t![](image/1.png) ## 修改网卡设置 > ​\tBOOTPROTO的值设为static > ``` vim /etc/sysconfig/network scripts/ifcfg eth0 ``` ​\t![image 20230607171429267](image/image 20230607171429267.png) ##### \t\t修改完后重启网卡 ``` service network restart ``` ​\t<img src \"image/2.png\" /> ## 主机映射路径 ##### \t\twindows下增加主机映射路径 ​\tC:\\Windows\\System32\\drivers\\etc\\hosts文件 ![](image/5.png) ​\t列如 192.168.56.47 LG04 ​\t![](image/3.png) ##### \t\tlinux下增加主机映射路径 ``` vim /etc/hosts ``` ​\t![](image/4.png) ##### \t在xshell中通过主机名链接主机 ​\t![image 20230607150923606](image/6.png)"},"/doc2/三、配置Java环境.html":{"title":"配置Java环境","content":"# 配置Java环境 ## 安装JDK ##### 在/opt下创建两个目录 ``` cd /opt/ mkdir ./apps mkdir ./soft cd soft ``` ​\t![](image/7.png) ##### 将jdk放入 /opt/soft目录中![](image/image 20230607152234807.png) ##### 解压到apps ``` tar zxvf jdk 8u181 linux x64.tar.gz C /opt/apps/ ``` ![](image/image 20230607152547352.png) ##### 进入opt下的jdk ``` cd /opt/apps/jdk1.8.0_181/ ``` ​\t![image 20230607152841625](image/image 20230607152841625.png) ##### 进入bin ``` cd ./bin ``` ​\t![image 20230607153056062](image/image 20230607153056062.png) > drwxrwxrwx\t第一位表示文件类型 ，2~4表示管理员权限，5~7表示组员权限，8~10表示所属者权限\tr:读\tw:改\tx:执行 ##### 查找需要卸载的jdk ``` rpm qagrep jdk ``` ​\t![image 20230607153659113](image/image 20230607153659113.png) ##### 卸载 ``` rpm e nodeps java 1.7.0 openjdk 1.7.0.79 2.5.5.4.el6.x86_64 rpm e nodeps java 1.6.0 openjdk 1.6.0.35 1.13.7.1.el6_6.x86_64 ``` ​\t![image 20230607153859607](image/image 20230607153859607.png) ## 配置环境变量 ##### 编辑profile文件 ``` vim /etc/profile ``` ​\t![image 20230607154537738](image/image 20230607154537738.png) ##### 在最下方添加以下内容 ``` export JAVA_HOME /opt/apps/jdk1.8.0_181 export PATH $PATH:$JAVA_HOME/bin ``` ​\t![image 20230607155306076](image/image 20230607155306076.png) ##### 刷新profile文件 ``` source /etc/profile ``` ![image 20230607155733278](image/image 20230607155733278.png)"},"/doc2/五、配置免密ssh.html":{"title":"配置免密ssh","content":"# 配置免密ssh > 每个服务器都要执行以下操作 ## 产生两把钥匙 > 一直按回车 ``` ssh keygen t rsa ``` ![image 20230608141214929](image/image 20230608141214929.png) ## 进入/root/下的.ssh ``` cd /root/.ssh/ ``` ![image 20230608141349798](image/image 20230608141349798.png) ## 将公密钥发给LG05、LG06、LG04(包括自己) > 自己也要发送，可以不用密码来访问这三个服务器 ``` ssh copy id LG05 ``` ![image 20230608141556277](image/image 20230608141556277.png) ## 每个服务器都要发送密钥（包括自己） ![image 20230608142058540](image/image 20230608142058540.png) ## 免密登录各个服务器 ``` ssh LG04 ssh LG05 ssh LG06 ``` ![image 20230608142344932](image/image 20230608142344932.png)"},"/doc2/四、克隆主机.html":{"title":"克隆主机","content":"# 克隆主机 ## 将LG04关机在VMware中按以下步骤操作 ​\t![image 20230607172213118](image/image 20230607172213118.png) ## 点击下一步 ​\t![image 20230607172302478](image/image 20230607172302478.png) ## 点击下一步 ​\t![image 20230607172331537](image/image 20230607172331537.png) ## 选择创建完整克隆，点击下一步 ​\t![image 20230607172423539](image/image 20230607172423539.png) ## 输入名称，地址，点击完成 ​\t![image 20230607172622328](image/image 20230607172622328.png) ## 开启LG06\t\t![image 20230607172744818](image/image 20230607172744818.png) ## 更改主机名为LG06 ``` vim /etc/sysconfig/network ``` ​\t![image 20230607173001070](image/image 20230607173001070.png) ## 删除NAME \"eth0\"旧网卡删除，将新网卡NAME \"eth1\"改为\"eth0\"，复制新的ipv6地址ATTR{address} \"ipv6的值\" ``` vim /etc/udev/rules.d/70 persistent net.rules ``` ​\t![image 20230607173619101](image/image 20230607173619101.png) ## 修改网卡配置，更改复制的ipv6地址，改静态ipv4地址 ``` vim /etc/sysconfig/network scripts/ifcfg eth0 ``` ​\t![image 20230607173841709](image/image 20230607173841709.png) ## 重启LG06 ``` reboot ``` ## ping www.baidu.com，发现能够ping通 ``` ping www.baidu.com ``` ​\t![image 20230607174120931](image/image 20230607174120931.png) ## 查看LG04、LG05、LG06IP地址 ``` ifconfig ``` ​\t![image 20230607174343609](image/image 20230607174343609.png) ​\t![image 20230607174405254](image/image 20230607174405254.png) ​\t![image 20230607174423811](image/image 20230607174423811.png) ## 使用xshell连接LG04、LG05、LG06 ​\t![image 20230607174832380](image/image 20230607174832380.png)"},"/doc2/七、Hive.html":{"title":"Hive","content":"# Hive > hive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。Hive的优点是学习成本低，可以通过类似SQL语句实现快速MapReduce统计，使MapReduce变得更加简单，而不必开发专门的MapReduce应用程序。hive十分适合对数据仓库进行统计分析。 ### hadoop文件操作 ##### 在LG04中启动dfs 在LG06中启动yarm ``` /opt/apps/hadoop 2.7.2/sbin/start dfs.sh /opt/apps/hadoop 2.7.2/sbin/start yarn.sh ``` ![image 20230610140839910](image/image 20230610140839910.png) ![image 20230610141413998](image/image 20230610141413998.png) ##### 在bin中创建文件夹test ``` ./hdfs dfs mkdir /test ``` ![image 20230610141556002](image/image 20230610141556002.png) ![image 20230610141620570](image/image 20230610141620570.png) ##### 在bin中上传文件 ``` ./hdfs dfs put /opt/apps/data.txt /test //其中的 put将put换成linux文件指令就可操作文件 ``` ![image 20230610142444823](image/image 20230610142444823.png) ![image 20230610142431601](image/image 20230610142431601.png) ### 安装hive ##### 将hive安装包放入/opt/soft ![image 20230610143344125](image/image 20230610143344125.png) ##### 解压到/opt/apps ``` tar zxvf ./apache hive 1.2.2 bin.tar.gz C /opt/apps/ ``` ![image 20230610143453022](image/image 20230610143453022.png) ##### 到解压的文件目录下 ![image 20230610143631621](image/image 20230610143631621.png) ### 安装Mysql ##### 找出rpm中原来的mysql包并卸载 ``` rpm qagrep mysql rpm e nodeps mysql libs 5.1.73 5.el6_6.x86_64 ``` ![image 20230610144924385](image/image 20230610144924385.png) ##### 将MySQL的客户端、服务端、驱动放入/opt/soft/中并安装 > 先安装服务端，在安装客户端，最后启动 > > mysql只启动这一次，以后不用再重启 ``` rpm ivh ./MySQL server 5.5.47 1.linux2.6.x86_64.rpm rpm ivh ./MySQL client 5.5.47 1.linux2.6.x86_64.rpm service mysql start ``` ![image 20230610150732374](image/image 20230610150732374.png) ![image 20230610145613650](image/image 20230610145613650.png) ##### 进入mysql ``` mysql ``` ![image 20230610153058295](image/image 20230610153058295.png) ``` show databases; use test; show tables; ``` ![image 20230610153316621](image/image 20230610153316621.png) ##### 设置mysql密码 ``` mysqladmin u root password \"root\" mysql uroot proot ``` ![image 20230610153438400](image/image 20230610153438400.png) ##### 使用mysql数据库中的user表 ``` show databases; use mysql; show tables; ``` ![image 20230619164358128](image/image 20230619164358128.png) ##### 查询user表中的host、user、password数据 > mysql用户登录的密码 ``` select host,user,password from user; ``` ![image 20230619164641999](image/image 20230619164641999.png) ##### 修改host为LG02的password值为localhost的password值 > 让root用户登录LG02 ``` update user set password \"*81F5E21E35407D884A6CD4A731AEBFB6AF209E1B\" where host \"lg02\" and user \"root\"; ``` ![image 20230619165142409](image/image 20230619165142409.png) ##### 刷新所有的权限 > 无论什么主机都能使用密码登录mysql ``` grant all privileges on *.* to 'root'@'%' identified by 'root'; flush privileges; ``` ![image 20230619165610030](image/image 20230619165610030.png) ``` cp /usr/share/mysql/my medium.cnf /etc/my.cnf vim /etc/my.cnf character_set_server utf8 ``` ![image 20230610154737778](image/image 20230610154737778.png) ``` service mysql restart ``` ![image 20230610154839207](image/image 20230610154839207.png) ##### 在Navicat中链接LG04中的mysql ![image 20230610155157814](image/image 20230610155157814.png) ### 配置Hive ##### 进入/opt/apps/apache hive 1.2.2 bin/conf目录下 ``` cd /opt/apps/apache hive 1.2.2 bin/conf ``` ##### 将hive env.sh.template重命名为hive env.sh并编辑 ``` mv ./hive env.sh.template ./hive env.sh vim hive env.sh ``` ![image 20230610155812018](image/image 20230610155812018.png) ##### 进入 vim hive default.xml.template复制头标签 ``` vim hive default.xml.template <?xml version \"1.0\" encoding \"UTF 8\" standalone \"no\"?> <?xml stylesheet type \"text/xsl\" href \"configuration.xsl\"?> ``` ##### 创建hive site.xml文件 ``` vim hive site.xml 将头文件复制在hive site.xml文件中 ``` ``` <?xml version \"1.0\" encoding \"UTF 8\" standalone \"no\"?> <?xml stylesheet type \"text/xsl\" href \"configuration.xsl\"?> <configuration> <property> <name>hive.metastore.uris</name> <value>thrift://LG04:9083</value> </property> <property> <name>hive.metastore.warehouse.dir</name> <value>/user/hive/warehouse</value> </property> <property> <name>hive.metastore.schema.verification</name> <value>false</value> </property> <property> <name>javax.jdo.option.ConnectionURL</name> <value>jdbc:mysql://LG04:3306/hive?createDatabaseIfNotExist true&amp;useUnicode true&amp;characterEncoding UTF 8</value> </property> <property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.jdbc.Driver</value> </property> <property> <name>javax.jdo.option.ConnectionUserName</name> <value>root</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>root</value> </property> <property> <name>hive.cli.print.current.db</name> <value>true</value> </property> <property> <name>hive.cli.print.header</name> <value>true</value> </property> </configuration> ``` ##### 进入/opt/apps/apache hive 1.2.2 bin/lib目录，并将驱动复制到该目录下 ``` cd /opt/apps/apache hive 1.2.2 bin/lib cp /opt/soft/mysql connector java 5.1.31.jar ./ ``` ![image 20230610181120681](image/image 20230610181120681.png) ##### 去/opt/apps/apache hive 1.2.2 bin/bin目录下执行./schematool dbType mysql initSchema ``` cd /opt/apps/apache hive 1.2.2 bin/bin ./schematool dbType mysql initSchema ``` ![image 20230610180618493](image/image 20230610180618493.png) ![image 20230610180659939](image/image 20230610180659939.png) ![image 20230610181230153](image/image 20230610181230153.png)"},"/doc2/项目.html":{"title":"项目","content":"# 项目 > 工具：navicat、 ### 数据库准备 ##### 链接数据库 ![image 20230614141341323](image/image 20230614141341323.png) ##### 创建数据库gmall1 ![image 20230614141458632](image/image 20230614141458632.png) ##### 执行sql脚本 ![image 20230614142053908](image/image 20230614142053908.png) ##### 数据库的分类 ![image 20230614142330053](image/image 20230614142330053.png) ##### 执行函数sql脚本 ![image 20230614142601934](image/image 20230614142601934.png) ##### 执行init_data函数 > 随机生成数据 > > do_date_string\t日期 > > order_incr_num\t订单 > > user_incr_num\t用户 > > sku_num\t商品 > > if_truncate\t0（追加）1（覆盖） ![image 20230614142727535](image/image 20230614142727535.png) ![image 20230614142755171](image/image 20230614142755171.png) ![image 20230614142840737](image/image 20230614142840737.png) ![image 20230614142911738](image/image 20230614142911738.png) ![image 20230614143036333](image/image 20230614143036333.png) ![image 20230614143059209](image/image 20230614143059209.png) ### 启动集群 ##### 在LG04中启动dfs 在LG06中启动yarm ``` /opt/apps/hadoop 2.7.2/sbin/start dfs.sh /opt/apps/hadoop 2.7.2/sbin/start yarn.sh ``` ##### 启动hive ``` cd /opt/apps/apache hive 1.2.2 bin/bin/ ./hive service metastore& ./hive ``` ![image 20230614152020397](image/image 20230614152020397.png) ##### 创建gmall1数据库 ``` create database gmall1; use gmall1; ``` ![image 20230614160915905](image/image 20230614160915905.png)"},"/doc2/视频网址.html":{"title":"网址：[高校实训平台](http://pt.1000phone.com/joinClassroom/2031/1686124573957)","content":"# 网址：[高校实训平台](http://pt.1000phone.com/joinClassroom/2031/1686124573957) # 用户名：15674567512 # 密码：lichao123"},"/doc2/index.html":{"title":"虚拟机的安装","content":"# 虚拟机的安装 ## 点击创建新的虚拟机 ![image 20230619135144223](image/image 20230619135144223.png) ## 选中典型并点击下一步 ![image 20230619135302422](image/image 20230619135302422.png) ## 选择稍后安装操作系统，再点击下一步 ![image 20230619135402804](image/image 20230619135402804.png) ## 选择Linux，版本选择CentOS 6 64位，再点击下一步 > 版本选择对应光驱版本 ![image 20230619135627964](image/image 20230619135627964.png) ## 输入虚拟机名称、位置，再点击下一步 ![image 20230619135829251](image/image 20230619135829251.png) ## 选择磁盘大小，将虚拟磁盘存储为单个文件，最后点击下一步 ![image 20230619140523720](image/image 20230619140523720.png) ## 点击完成 ![image 20230619140632608](image/image 20230619140632608.png) ## 点击编辑虚拟机设置 ![image 20230619140722369](image/image 20230619140722369.png) ## 虚拟机内存设置为2048 ![image 20230619140852540](image/image 20230619140852540.png) ## 在CD设置中选择使用ISO映像文件，选择ISO文件路径 ![image 20230619141121512](image/image 20230619141121512.png) ## 可以选择将打印机移除 ![image 20230619141234696](image/image 20230619141234696.png) ## 配置完成后点击确定 ![image 20230619141309315](image/image 20230619141309315.png) ## 开启LG01虚拟机 ![image 20230619141342690](image/image 20230619141342690.png) ## 选择第一个，并按回车 ![image 20230619141456263](image/image 20230619141456263.png) ## 通过按左右键将光标移动到Skip，并按回车跳过检测 ![image 20230619141554958](image/image 20230619141554958.png) ## 点击Next ![image 20230619141635289](image/image 20230619141635289.png) ## 选择英语，再点击Next ![image 20230619141736786](image/image 20230619141736786.png) ## 继续点击Next ![image 20230619141826527](image/image 20230619141826527.png) ## 选择基础的安装，点击下一步 ![image 20230619141922005](image/image 20230619141922005.png) ## 选择Yes，忽略所有数据 ![image 20230619142011038](image/image 20230619142011038.png) ## 虚拟机主机名和虚拟机名保持一致，为LG01，点击Configure Network，选中System eth0，点击Edit。 ![image 20230619142529078](image/image 20230619142529078.png) ## 返回真实主机，进入网络连接，选中VMware Network Adapter VMnet8，并点击鼠标右键中的属性 ![image 20230619143006575](image/image 20230619143006575.png) ## 选中ipv4，点击属性，并输入以下配置 ![image 20230619143115294](image/image 20230619143115294.png) ## 勾选Connect automatically，点击IPv4 Settings，配置好网络地址、子网掩码、网关、DNS，最后点击apply ![image 20230619143855295](image/image 20230619143855295.png) ## 点击Close，再点击Next ![image 20230619143950122](image/image 20230619143950122.png) ## 选择上海时区，再点击Next ![image 20230619144130567](image/image 20230619144130567.png) ## 这里输入root密码，点击下一步 ![image 20230619144341313](image/image 20230619144341313.png) ## 选择Use Anyway ![image 20230619144425559](image/image 20230619144425559.png) ## 选择Create Custom Layout，点击Next ![image 20230619144539497](image/image 20230619144539497.png) ## 点击Create ![image 20230619144952112](image/image 20230619144952112.png) ## 依次创建/boot、交换空间swap、根目录 ![image 20230619145043521](image/image 20230619145043521.png) ## 点击Next、Format ![image 20230619145135841](image/image 20230619145135841.png) ## 点击write changes to disk ![image 20230619145216471](image/image 20230619145216471.png) ## 接下来的步骤一直点Next，等待安装 **![image 20230619145327531](image/image 20230619145327531.png)** ## 点击Reboot重启 ![image 20230619150112039](image/image 20230619150112039.png) ## 欢迎界面点击Forward ![image 20230619150320863](image/image 20230619150320863.png) ## 同意条约，点击Forward ![image 20230619150404409](image/image 20230619150404409.png) ## 不创建普通用户，直接点击Forward，Yes ![image 20230619150550554](image/image 20230619150550554.png) ## 选择与互联网同步时间，点击Forward ![image 20230619150821036](image/image 20230619150821036.png) ## 将kdump选项去掉，直接点击Finish ![image 20230619151016291](image/image 20230619151016291.png) ## 在VMware中配置虚拟网络编辑器 ![image 20230619153254263](image/image 20230619153254263.png)"},"/doc2/草稿.html":{"title":"","content":"![image 20230620110336071](image/image 20230620110336071.png) ##### hive ``` drop table if exists `gmall1.ods_order_info`; create external table `gmall1.ods_order_info` ( `id` string COMMENT '订单号', `order_status` string COMMENT '订单状态', `user_id` string COMMENT '用户id', `payment_way` string COMMENT '支付方式', `out_trade_no` string COMMENT '支付流水号', `create_time` string COMMENT '创建时间', `operate_time` string COMMENT '操作时间' ) COMMENT '订单表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/order_info/data'; ``` ``` drop table if exists `gmall1.ods_order_detail`; create external table `gmall1.ods_order_detail`( `id` string COMMENT '订单编号', `order_id` string COMMENT '订单号', `user_id` string COMMENT '用户id', `sku_id` string COMMENT '商品id', `sku_num` bigint COMMENT '商品数量', `create_time` string COMMENT '创建时间' ) COMMENT '订单详情表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/order_detail/data'; ``` ``` drop table if exists `gmall1.ods_sku_info`; create external table `gmall1.ods_sku_info`( `id` string COMMENT 'skuId', `spu_id` string COMMENT 'spuid', `price` decimal(10,2) COMMENT '价格', `sku_name` string COMMENT '商品名称', `sku_desc` string COMMENT '商品描述', `weight` string COMMENT '重量', `tm_id` string COMMENT '品牌id', `category3_id` string COMMENT '品类id', `create_time` string COMMENT '创建时间' ) COMMENT 'SKU商品表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/sku_info/data'; ``` ``` drop table if exists `gmall1.ods_user_info`; create external table `gmall1.ods_user_info`( `id` string COMMENT '用户id', `name` string COMMENT '姓名', `birthday` string COMMENT '生日', `gender` string COMMENT '性别', `email` string COMMENT '邮箱', `user_level` string COMMENT '用户等级', `create_time` string COMMENT '创建时间' ) COMMENT '用户表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/user_info/data'; ``` ``` drop table if exists `gmall1.ods_base_category1`; create external table `gmall1.ods_base_category1`( `id` string COMMENT 'id', `name` string COMMENT '名称' ) COMMENT '商品一级分类表' row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/base_category1/data/data1'; ``` ``` drop table if exists `gmall1.ods_base_category2`; create external table `gmall1.ods_base_category2`( `id` string COMMENT 'id', `name` string COMMENT '名称', `category1_id` string COMMENT '一级品类id' ) COMMENT '商品二级分类表' row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/base_category2/data/data1'; ``` ``` drop table if exists `gmall1.ods_base_category3`; create external table `gmall1.ods_base_category3`( `id` string COMMENT ' id', `name` string COMMENT '名称', `category2_id` string COMMENT '二级品类id' ) COMMENT '商品三级分类表' row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/base_category3/data/data1'; ``` ``` drop table if exists `gmall1.ods_payment_info`; create external table `gmall1.ods_payment_info`( `id` bigint COMMENT '编号', `out_trade_no` string COMMENT '对外业务编号', `order_id` string COMMENT '订单编号', `user_id` string COMMENT '用户编号', `alipay_trade_no` string COMMENT '支付宝交易流水编号', `subject` string COMMENT '交易内容', `payment_type` string COMMENT '支付类型', `payment_time` string COMMENT '支付时间' ) COMMENT '支付流水表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/payment_info/data'; ``` ##### shell脚本 ``` ./sqoop import \\ connect jdbc:mysql://LG02:3306/gmall1?characterEncoding utf 8 \\ username root \\ password root \\ query \"$2 and \\$CONDITIONS\" m 1 \\ target dir /gmall1_data/$1/data/$do_date \\ delete target dir \\ fields terminated by \",\" \\ lines terminated by \"\\n\" \\ null string '\\\\N' \\ null non string '\\\\N'; ``` ``` #! /bin/bash sqoop /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin/sqoop do_date `date d ' 1 day' +%F` if [ n \"$2\" ]; then do_date $2 fi import_data(){ $sqoop import \\ connect jdbc:mysql://LG02:3306/gmall1?characterEncoding utf 8 \\ username root \\ password root \\ query \"$2 and \\$CONDITIONS\" m 1 \\ target dir /gmall1_data/$1/data/$do_date \\ delete target dir \\ fields terminated by \",\" \\ lines terminated by \"\\n\" \\ null string '\\\\N' \\ null non string '\\\\N'; } import_order_info(){ import_data \"order_info\" \"select id, order_status, user_id, payment_way, out_trade_no, create_time, operate_time from order_info where (date_format(create_time,'%Y %m %d') '$do_date' or date_format(operate_time,'%Y %m %d') '$do_date')\" } import_user_info(){ import_data \"user_info\" \"select id, name, birthday, gender, email, user_level, create_time from user_info where DATE_FORMAT(create_time,'%Y %m %d') '$do_date'\" } import_order_detail(){ import_data \"order_detail\" \"select od.id, order_id, user_id, sku_id, sku_num, od.create_time from order_detail od join order_info oi on od.order_id oi.id where DATE_FORMAT(od.create_time,'%Y %m %d') '$do_date'\" } import_payment_info(){ import_data \"payment_info\" \"select id, out_trade_no, order_id, user_id, alipay_trade_no, subject, payment_type, payment_time from payment_info where DATE_FORMAT(payment_time,'%Y %m %d') '$do_date'\" } import_sku_info(){ import_data \"sku_info\" \"select id, spu_id, price, sku_name, sku_desc, weight, tm_id, category3_id, create_time from sku_info where DATE_FORMAT(create_time,'%Y %m %d') '$do_date'\" } import_base_category1(){ import_data \"base_category1\" \"select id, name from base_category1 where 1 1\" } import_base_category2(){ import_data \"base_category2\" \"select id, name, category1_id from base_category2 where 1 1\" } import_base_category3(){ import_data \"base_category3\" \"select id, name, category2_id from base_category3 where 1 1\" } case $1 in \"order_info\") import_order_info ;; \"base_category1\") import_base_category1 ;; \"base_category2\") import_base_category2 ;; \"base_category3\") import_base_category3 ;; \"order_detail\") import_order_detail ;; \"sku_info\") import_sku_info ;; \"user_info\") import_user_info ;; \"payment_info\") import_payment_info ;; \"first\") import_base_category1 import_base_category2 import_base_category3 import_order_info import_order_detail import_sku_info import_user_info import_payment_info ;; \"all\") import_order_info import_order_detail import_sku_info import_user_info import_payment_info ;; esac ``` ``` ./sqoop import \\ connect jdbc:mysql://LG02:3306/gmall1?characterEncoding utf 8 \\ username root \\ password root \\ query \"select id,order_status,user_id,payment_way,out_trade_no,create_time,operate_time from order_info where (date_format(create_time,'%Y %m %d') '2023 06 13' or date_format(operate_time,'%Y %m %d') '2023 06 13') and \\$CONDITIONS\" m 1 \\ target dir /gmall1_data/order_info/data/2023 06 13 \\ fields terminated by \",\" \\ lines terminated by \"\\n\" \\ null string '\\\\N' \\ null non string '\\\\N'; ``` ``` ./sqoop import connect jdbc:mysql://LG02:3306/test username root password root table base_category1 target dir /data/to_hdfs_nba_key check column id incremental append last value 4 ``` ``` load data inpath '/gmall1_data/order_info/data/2023 06 13' OVERWRITE into table `gmall1.ods_order_info` partition(`dt` '2023 06 13'); ``` ``` ./sqoop import \\ connect jdbc:mysql://LG:3306/gmall1?characterEncoding utf 8 \\ username root \\ password root \\ query \"select id,name from base_category1 where 1 1 and \\$CONDITIONS\" m 1 \\ target dir /gmall1_data/base_category1/data/2020 07 25 \\ fields terminated by \",\" \\ lines terminated by \"\\n\" \\ null string '\\\\N' \\ null non string '\\\\N'; ``` ``` #!/bin/bash APP gmall1 hive /opt/apps/apache hive 1.2.2 bin/bin/hive # 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天 if [ n \"$2\" ]; then do_date $2 else do_date `date d \" 1 day\" +%F` fi sql1 \" load data inpath '/gmall1_data/order_info/data/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt '$do_date'); load data inpath '/gmall1_data/order_detail/data/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt '$do_date'); load data inpath '/gmall1_data/sku_info/data/$do_date' OVERWRITE into table ${APP}.ods_sku_info partition(dt '$do_date'); load data inpath '/gmall1_data/user_info/data/$do_date' OVERWRITE into table ${APP}.ods_user_info partition(dt '$do_date'); load data inpath '/gmall1_data/payment_info/data/$do_date' OVERWRITE into table ${APP}.ods_payment_info partition(dt '$do_date'); load data inpath '/gmall1_data/base_category1/data/$do_date' OVERWRITE into table ${APP}.ods_base_category1; load data inpath '/gmall1_data/base_category2/data/$do_date' OVERWRITE into table ${APP}.ods_base_category2; load data inpath '/gmall1_data/base_category3/data/$do_date' OVERWRITE into table ${APP}.ods_base_category3; \" sql2 \" load data inpath '/gmall1_data/order_info/data/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt '$do_date'); load data inpath '/gmall1_data/order_detail/data/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt '$do_date'); load data inpath '/gmall1_data/sku_info/data/$do_date' OVERWRITE into table ${APP}.ods_sku_info partition(dt '$do_date'); load data inpath '/gmall1_data/user_info/data/$do_date' OVERWRITE into table ${APP}.ods_user_info partition(dt '$do_date'); load data inpath '/gmall1_data/payment_info/data/$do_date' OVERWRITE into table ${APP}.ods_payment_info partition(dt '$do_date'); \" case $1 in \"first\") $hive e \"$sql1\" ;; \"all\") $hive e \"$sql2\" ;; esac ``` ``` insert overwrite table `gmall1.dwd_order_info` partition(`dt` '2023 06 15') select oi.`id`, s4.`name`, oi.`order_status`, oi.`user_id`, oi.`payment_way`, oi.`out_trade_no`, oi.`create_time`, oi.`operate_time` from (select * from `gmall1.ods_order_info` where `dt` '2023 06 15' and `id` is not null) oi left join (select `order_id`,`sku_id`,`sku_num` from `gmall1.ods_order_detail` where `dt` '2023 06 15' and `id` is not null) od on oi.`id` od.`order_id` left join (select `id`,`category3_id` from `gmall1.ods_sku_info` where `id` is not null) s1 on od.`sku_id` s1.`id` left join (select `id` from `gmall1.ods_base_category3` where `id` is not null) s2 on s1.`category3_id` s2.`id` left join (select `id` from `gmall1.ods_base_category2` where `id` is not null) s3 on s2.`category2_id` s3.`id` left join (select `id`,`name` from `gmall1.ods_base_category1` where `id` is not null) s4 on s3.`category1_id` s4.`id` group by oi.`id`, oi.`order_status`, oi.`user_id`, oi.`payment_way`, oi.`out_trade_no`, oi.`create_time`, oi.`operate_time`; ```"},"/doc2/01.html":{"title":"虚拟机的安装","content":" title: 虚拟机的安装 date: 2025 09 25 # 虚拟机的安装 ##### 点击创建新的虚拟机 ![image 20230619135144223](image/image 20230619135144223.png) ##### 选中典型并点击下一步 ![image 20230619135302422](image/image 20230619135302422.png) ##### 选择稍后安装操作系统，再点击下一步 ![image 20230619135402804](image/image 20230619135402804.png) ##### 选择Linux，版本选择CentOS 6 64位，再点击下一步 > 版本选择对应光驱版本 ![image 20230619135627964](image/image 20230619135627964.png) ##### 输入虚拟机名称、位置，再点击下一步 ![image 20230619135829251](image/image 20230619135829251.png) ##### 选择磁盘大小，将虚拟磁盘存储为单个文件，最后点击下一步 ![image 20230619140523720](image/image 20230619140523720.png) ##### 点击完成 ![image 20230619140632608](image/image 20230619140632608.png) ##### 点击编辑虚拟机设置 ![image 20230619140722369](image/image 20230619140722369.png) ##### 虚拟机内存设置为2048 ![image 20230619140852540](image/image 20230619140852540.png) ##### 在CD设置中选择使用ISO映像文件，选择ISO文件路径 ![image 20230619141121512](image/image 20230619141121512.png) ##### 可以选择将打印机移除 ![image 20230619141234696](image/image 20230619141234696.png) ##### 配置完成后点击确定 ![image 20230619141309315](image/image 20230619141309315.png) ##### 开启LG01虚拟机 ![image 20230619141342690](image/image 20230619141342690.png) ##### 选择第一个，并按回车 ![image 20230619141456263](image/image 20230619141456263.png) ##### 通过按左右键将光标移动到Skip，并按回车跳过检测 ![image 20230619141554958](image/image 20230619141554958.png) ##### 点击Next ![image 20230619141635289](image/image 20230619141635289.png) ##### 选择英语，再点击Next ![image 20230619141736786](image/image 20230619141736786.png) ##### 继续点击Next ![image 20230619141826527](image/image 20230619141826527.png) ##### 选择基础的安装，点击下一步 ![image 20230619141922005](image/image 20230619141922005.png) ##### 选择Yes，忽略所有数据 ![image 20230619142011038](image/image 20230619142011038.png) ##### 虚拟机主机名和虚拟机名保持一致，为LG01，点击Configure Network，选中System eth0，点击Edit。 ![image 20230619142529078](image/image 20230619142529078.png) ##### 返回真实主机，进入网络连接，选中VMware Network Adapter VMnet8，并点击鼠标右键中的属性 ![image 20230619143006575](image/image 20230619143006575.png) ##### 选中ipv4，点击属性，并输入以下配置 ![image 20230619143115294](image/image 20230619143115294.png) ##### 勾选Connect automatically，点击IPv4 Settings，配置好网络地址、子网掩码、网关、DNS，最后点击apply ![image 20230619143855295](image/image 20230619143855295.png) ##### 点击Close，再点击Next ![image 20230619143950122](image/image 20230619143950122.png) ##### 选择上海时区，再点击Next ![image 20230619144130567](image/image 20230619144130567.png) ##### 这里输入root密码，点击下一步 ![image 20230619144341313](image/image 20230619144341313.png) ##### 选择Use Anyway ![image 20230619144425559](image/image 20230619144425559.png) ##### 选择Create Custom Layout，点击Next ![image 20230619144539497](image/image 20230619144539497.png) ##### 点击Create ![image 20230619144952112](image/image 20230619144952112.png) ##### 依次创建/boot、交换空间swap、根目录 ![image 20230619145043521](image/image 20230619145043521.png) ##### 点击Next、Format ![image 20230619145135841](image/image 20230619145135841.png) ##### 点击write changes to disk ![image 20230619145216471](image/image 20230619145216471.png) ##### 接下来的步骤一直点Next，等待安装 **![image 20230619145327531](image/image 20230619145327531.png)** ##### 点击Reboot重启 ![image 20230619150112039](image/image 20230619150112039.png) ##### 欢迎界面点击Forward ![image 20230619150320863](image/image 20230619150320863.png) ##### 同意条约，点击Forward ![image 20230619150404409](image/image 20230619150404409.png) ##### 不创建普通用户，直接点击Forward，Yes ![image 20230619150550554](image/image 20230619150550554.png) ##### 选择与互联网同步时间，点击Forward ![image 20230619150821036](image/image 20230619150821036.png) ##### 将kdump选项去掉，直接点击Finish ![image 20230619151016291](image/image 20230619151016291.png) ##### 在VMware中配置虚拟网络编辑器 ![image 20230619153254263](image/image 20230619153254263.png)"},"/doc2/实训项目随笔.html":{"title":"第一步、ODS层，创建hive表，将mysql数据导入ODS层","content":"# 第一步、ODS层，创建hive表，将mysql数据导入ODS层 ### 1、hive操作 ##### 1.1 创建数据库 ``` create databases gmall1; ``` ##### 1.2 使用数据库 ``` use gmall1; ``` ##### 1.2 订单表（增量及更新） ``` drop table if exists `gmall1.ods_order_info`; create external table `gmall1.ods_order_info` ( `id` string COMMENT '订单号', `order_status` string COMMENT '订单状态', `user_id` string COMMENT '用户id', `payment_way` string COMMENT '支付方式', `out_trade_no` string COMMENT '支付流水号', `create_time` string COMMENT '创建时间', `operate_time` string COMMENT '操作时间' ) COMMENT '订单表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/order_info/data'; ``` ##### 1.3 订单详情表（增量及更新） ``` drop table if exists `gmall1.ods_order_detail`; create external table `gmall1.ods_order_detail`( `id` string COMMENT '订单编号', `order_id` string COMMENT '订单号', `user_id` string COMMENT '用户id', `sku_id` string COMMENT '商品id', `sku_num` bigint COMMENT '商品数量', `create_time` string COMMENT '创建时间' ) COMMENT '订单详情表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/order_detail/data'; ``` ##### 1.4 SKU商品表（增量及更新） ``` drop table if exists `gmall1.ods_sku_info`; create external table `gmall1.ods_sku_info`( `id` string COMMENT 'skuId', `spu_id` string COMMENT 'spuid', `price` decimal(10,2) COMMENT '价格', `sku_name` string COMMENT '商品名称', `sku_desc` string COMMENT '商品描述', `weight` string COMMENT '重量', `tm_id` string COMMENT '品牌id', `category3_id` string COMMENT '品类id', `create_time` string COMMENT '创建时间' ) COMMENT 'SKU商品表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/sku_info/data'; ``` ##### 1.5 用户表（增量及更新） ``` drop table if exists `gmall1.ods_user_info`; create external table `gmall1.ods_user_info`( `id` string COMMENT '用户id', `name` string COMMENT '姓名', `birthday` string COMMENT '生日', `gender` string COMMENT '性别', `email` string COMMENT '邮箱', `user_level` string COMMENT '用户等级', `create_time` string COMMENT '创建时间' ) COMMENT '用户表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/user_info/data'; ``` ##### 1.6 商品一级分类表（全量） ``` drop table if exists `gmall1.ods_base_category1`; create external table `gmall1.ods_base_category1`( `id` string COMMENT 'id', `name` string COMMENT '名称' ) COMMENT '商品一级分类表' row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/base_category1/data/data1'; ``` ##### 1.7 商品二级分类表（全量） ``` drop table if exists `gmall1.ods_base_category2`; create external table `gmall1.ods_base_category2`( `id` string COMMENT 'id', `name` string COMMENT '名称', `category1_id` string COMMENT '一级品类id' ) COMMENT '商品二级分类表' row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/base_category2/data/data1'; ``` ##### 1.8 商品三级分类表（全量） ``` drop table if exists `gmall1.ods_base_category3`; create external table `gmall1.ods_base_category3`( `id` string COMMENT ' id', `name` string COMMENT '名称', `category2_id` string COMMENT '二级品类id' ) COMMENT '商品三级分类表' row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/base_category3/data/data1'; ``` ##### 1.9 支付流水表（增量及更新） ``` drop table if exists `gmall1.ods_payment_info`; create external table `gmall1.ods_payment_info`( `id` bigint COMMENT '编号', `out_trade_no` string COMMENT '对外业务编号', `order_id` string COMMENT '订单编号', `user_id` string COMMENT '用户编号', `alipay_trade_no` string COMMENT '支付宝交易流水编号', `subject` string COMMENT '交易内容', `payment_type` string COMMENT '支付类型', `payment_time` string COMMENT '支付时间' ) COMMENT '支付流水表' PARTITIONED BY (`dt` string) row format delimited fields terminated by ',' STORED AS TEXTFILE location '/gmall1_data/payment_info/data'; ``` ### 2、通过脚本、sqoop将数据导入hdfs中 ##### 2.1 对于脚本的编写，切记一定要让脚本为linux下的编码格式 ``` set fileformat unix ``` ##### 2.2 在/opt/apps/gmall1/gmall1_sh目录下创建 ``` vim mysql_to_hdfs.sh ``` ``` #! /bin/bash sqoop /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin/sqoop do_date `date d ' 1 day' +%F` if [ n \"$2\" ]; then do_date $2 fi import_data(){ $sqoop import \\ connect jdbc:mysql://LG04:3306/gmall1?characterEncoding utf 8 \\ username root \\ password root \\ query \"$2 and \\$CONDITIONS\" m 1 \\ target dir /gmall1_data/$1/data/$do_date \\ delete target dir \\ fields terminated by \",\" \\ lines terminated by \"\\n\" \\ null string '\\\\N' \\ null non string '\\\\N'; } import_order_info(){ import_data \"order_info\" \"select id, order_status, user_id, payment_way, out_trade_no, create_time, operate_time from order_info where (date_format(create_time,'%Y %m %d') '$do_date' or date_format(operate_time,'%Y %m %d') '$do_date')\" } import_user_info(){ import_data \"user_info\" \"select id, name, birthday, gender, email, user_level, create_time from user_info where DATE_FORMAT(create_time,'%Y %m %d') '$do_date'\" } import_order_detail(){ import_data \"order_detail\" \"select od.id, order_id, user_id, sku_id, sku_num, od.create_time from order_detail od join order_info oi on od.order_id oi.id where DATE_FORMAT(od.create_time,'%Y %m %d') '$do_date'\" } import_payment_info(){ import_data \"payment_info\" \"select id, out_trade_no, order_id, user_id, alipay_trade_no, subject, payment_type, payment_time from payment_info where DATE_FORMAT(payment_time,'%Y %m %d') '$do_date'\" } import_sku_info(){ import_data \"sku_info\" \"select id, spu_id, price, sku_name, sku_desc, weight, tm_id, category3_id, create_time from sku_info where DATE_FORMAT(create_time,'%Y %m %d') '$do_date'\" } import_base_category1(){ import_data \"base_category1\" \"select id, name from base_category1 where 1 1\" } import_base_category2(){ import_data \"base_category2\" \"select id, name, category1_id from base_category2 where 1 1\" } import_base_category3(){ import_data \"base_category3\" \"select id, name, category2_id from base_category3 where 1 1\" } case $1 in \"order_info\") import_order_info ;; \"base_category1\") import_base_category1 ;; \"base_category2\") import_base_category2 ;; \"base_category3\") import_base_category3 ;; \"order_detail\") import_order_detail ;; \"sku_info\") import_sku_info ;; \"user_info\") import_user_info ;; \"payment_info\") import_payment_info ;; \"first\") import_base_category1 import_base_category2 import_base_category3 import_order_info import_order_detail import_sku_info import_user_info import_payment_info ;; \"all\") import_order_info import_order_detail import_sku_info import_user_info import_payment_info ;; esac ``` ##### 2.3 修改脚本权限 ``` chmod 755 mysql_to_hdfs.sh ``` ##### 2.4 将每天的数据导入hdfs ``` mysql_to_hdfs.sh first 2023 06 13 mysql_to_hdfs.sh all 2023 06 13 ``` ### 3、通过脚本将hdfs的数据导入ODS层 ##### 3.1 在/opt/apps/gmall1/gmall1_sh目录下创建脚本hdfs_to_ods_db.sh ``` vim hdfs_to_ods_db.sh ``` ``` #!/bin/bash APP gmall1 hive /opt/apps/apache hive 1.2.2 bin/bin/hive # 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天 if [ n \"$2\" ]; then do_date $2 else do_date `date d \" 1 day\" +%F` fi sql1 \" load data inpath '/gmall1_data/order_info/data/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt '$do_date'); load data inpath '/gmall1_data/order_detail/data/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt '$do_date'); load data inpath '/gmall1_data/sku_info/data/$do_date' OVERWRITE into table ${APP}.ods_sku_info partition(dt '$do_date'); load data inpath '/gmall1_data/user_info/data/$do_date' OVERWRITE into table ${APP}.ods_user_info partition(dt '$do_date'); load data inpath '/gmall1_data/payment_info/data/$do_date' OVERWRITE into table ${APP}.ods_payment_info partition(dt '$do_date'); load data inpath '/gmall1_data/base_category1/data/$do_date' OVERWRITE into table ${APP}.ods_base_category1; load data inpath '/gmall1_data/base_category2/data/$do_date' OVERWRITE into table ${APP}.ods_base_category2; load data inpath '/gmall1_data/base_category3/data/$do_date' OVERWRITE into table ${APP}.ods_base_category3; \" sql2 \" load data inpath '/gmall1_data/order_info/data/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt '$do_date'); load data inpath '/gmall1_data/order_detail/data/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt '$do_date'); load data inpath '/gmall1_data/sku_info/data/$do_date' OVERWRITE into table ${APP}.ods_sku_info partition(dt '$do_date'); load data inpath '/gmall1_data/user_info/data/$do_date' OVERWRITE into table ${APP}.ods_user_info partition(dt '$do_date'); load data inpath '/gmall1_data/payment_info/data/$do_date' OVERWRITE into table ${APP}.ods_payment_info partition(dt '$do_date'); \" case $1 in \"first\") $hive e \"$sql1\" ;; \"all\") $hive e \"$sql2\" ;; esac ``` ##### 3.2 修改脚本权限 ``` chmod 755 hdfs_to_ods_db.sh ``` ##### 3.3 将每天的数据导入ODS ``` ./hdfs_to_ods_db.sh first 2023 06 13 ./hdfs_to_ods_db.sh all 2023 06 13 ```"},"/doc2/九、sqoop安装.html":{"title":"sqoop安装","content":"# sqoop安装 > Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个Apache项目。 ##### 将sqoop放入/opt/soft目录中 ![image 20230612162120783](image/image 20230612162120783.png) ##### 解压缩到/opt/apps目录中 ``` tar zxvf ./sqoop 1.4.7.bin__hadoop 2.6.0.tar.gz C /opt/apps/ ``` ![image 20230612162241314](image/image 20230612162241314.png) ##### 进入/opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/conf/目录 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/conf/ ``` ##### 将 sqoop env template.sh修改为 sqoop env.sh ``` mv ./sqoop env template.sh ./sqoop env.sh ``` ##### 修改sqoop env.sh文件 ``` vim sqoop env.sh ``` ![image 20230613140536089](image/image 20230613140536089.png) ##### 将mysql的jar包拷贝到sqoop的lib目录下 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/ ll ``` ![image 20230613140858226](image/image 20230613140858226.png) ##### 在sqoop的bin目录下执行以下操作 ``` cd /opt/apps/sqoop 1.4.7.bin__hadoop 2.6.0/bin ./sqoop list databases connect jdbc:mysql://LG04:3306 username root password root ``` ![image 20230613141639539](image/image 20230613141639539.png)"}}